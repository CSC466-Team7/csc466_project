(this["webpackJsonpcsc466-education-app"]=this["webpackJsonpcsc466-education-app"]||[]).push([[0],{200:function(e,t,n){},349:function(e,t,n){},582:function(e,t,n){},584:function(e,t,n){"use strict";n.r(t);var i=n(0),a=n.n(i),r=n(26),o=n.n(r),s=(n(200),n(16)),c=n(13),l=n(9),d=n(188),h=n(621),j=n(622),u=n(77),b=n(106),m=n.n(b),p=n(168),g=n(40),x=n(587),f=n(609),O=n(607),w=n(625),y=n(191),k=n(610),v=n(611),T=n(178),N=n.n(T),C=n(179),_=n.n(C),W=(n(202),n(623)),D=n(169),S=n(170),q=n.n(S),A=n(624),I=n(187),L=n(171),F=n.n(L),z=n(172),P=n.n(z),E=n(173),B=n.n(E),H=n(107),R=n.n(H),G=(!R.a.env.NODE_ENV||R.a.env.NODE_ENV,"https://raw.githubusercontent.com/CSC466-Team7/csc466_project/main"),V="".concat(G,"/code/markdown"),J="".concat(G,"/code/notebooks"),U="".concat(G,"/datasets"),M=n(190),Y=n(605),Z=n(626),K=n(1),X=Object(Y.a)((function(e){return Object(Z.a)({article:{margin:"25px auto",padding:"25px",width:"80%",backgroundColor:"whitesmoke"}})}));function Q(e){var t=X();return Object(K.jsx)(M.a,{elevation:3,className:t.article,children:e.children})}var $=Object(Y.a)((function(e){return Object(Z.a)({})}));function ee(e){$();return Object(K.jsx)(K.Fragment,{children:Object(K.jsxs)("section",{style:{width:"50%",margin:"20px auto"},children:[Object(K.jsx)("h1",{children:e.title}),Object(K.jsx)(y.a,{variant:"subtitle1",children:e.description}),Object(K.jsx)("hr",{})]})})}n(349);var te=n(608),ne=Object(Y.a)((function(e){return Object(Z.a)({wrapper:{width:"80%",margin:"10px auto",padding:"20px",position:"relative"},header:{maxWidth:"80%"},question:{maxWidth:"80%"},answer:{margin:"10px auto",padding:"20px",backgroundColor:"whitesmoke"},toggleControl:{position:"absolute",right:"0",top:"20px",maxWidth:"20%"}})}));function ie(e){return Object(K.jsx)(ae,{question:Object(K.jsx)(y.a,{variant:"h6",children:e.question}),answer:Object(K.jsx)(y.a,{children:e.answer})})}function ae(e){var t=ne(),n=Object(i.useState)(!1),a=Object(g.a)(n,2),r=a[0],o=a[1];return Object(K.jsxs)(M.a,{style:e.styleOverrides,className:t.wrapper,variant:"outlined",children:[Object(K.jsx)("div",{className:t.header,children:e.header}),Object(K.jsx)(O.a,{className:t.toggleControl,control:Object(K.jsx)(te.a,{checked:r,onChange:function(){return o(!r)},name:"showAnswer"}),label:"Show Answer"}),Object(K.jsx)("div",{className:t.question,children:e.question}),r&&Object(K.jsx)(M.a,{className:t.answer,variant:"outlined",children:e.answer})]})}function re(e){var t=Object(i.useState)(!0),n=Object(g.a)(t,2),a=n[0],r=n[1];return Object(K.jsxs)("div",{className:"collapsable",children:[Object(K.jsx)(x.a,{className:"collapse-btn",onClick:function(){r(!a)},children:a?Object(K.jsx)(_.a,{}):Object(K.jsx)(N.a,{})}),!a&&Object(K.jsx)("code",{className:"hidden-text",children:"Hidden"}),Object(K.jsx)(W.a,{in:a,children:Object(K.jsx)("div",{className:"collapse-custom",children:Object(K.jsx)("code",Object(s.a)({className:e.className,children:e.children},e))})})]})}function oe(e){return Object(K.jsxs)(f.a,{component:"a",color:"primary",variant:"contained",href:e.href,style:{margin:10},target:"_blank",download:!0,children:[Object(K.jsx)(k.a,{}),e.children]})}var se=function(e){return Object(K.jsx)(ae,{question:e.question,answer:e.answer,header:Object(K.jsx)("h3",{className:"mt-0",children:Object(K.jsx)("u",{children:"Activity: Check Your Knowledge!"})}),styleOverrides:{width:"100%",marginTop:"5vh",marginBottom:"5vh"}})};function ce(e){var t=Object(i.useState)(""),n=Object(g.a)(t,2),a=n[0],r=n[1],o=Object(i.useState)(!0),c=Object(g.a)(o,2),l=c[0],d=c[1],h=function(){var e=Object(p.a)(m.a.mark((function e(){var t,n,i,a;return m.a.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return t=l?j():b(),n=t,e.next=4,fetch(n);case 4:return i=e.sent,e.next=7,i.text();case 7:a=e.sent,r(a);case 9:case"end":return e.stop()}}),e)})));return function(){return e.apply(this,arguments)}}(),j=function(){return"".concat(V,"/").concat(e.fileName,"_cleaned.md")},b=function(){return"".concat(V,"/").concat(e.fileName,"_original.md")};Object(i.useEffect)((function(){h()}),[l]);var f={code:function(e){e.node;var t=e.inline,n=e.className,i=e.children,a=Object(u.a)(e,["node","inline","className","children"]),r=/language-(\w+)/.exec(n||"");return!t&&r?Object(K.jsxs)("span",{children:[Object(K.jsx)(D.CopyToClipboard,{text:String(i),children:Object(K.jsx)(x.a,{color:"primary",component:"span",style:{float:"right",marginLeft:10},children:Object(K.jsx)(v.a,{})})}),Object(K.jsx)(A.a,Object(s.a)({style:I.a,language:r[1],PreTag:"div",children:String(i).replace(/\n$/,"")},a))]}):t||r?Object(K.jsx)("code",Object(s.a)({className:n,children:i},a)):Object(K.jsx)(re,Object(s.a)({className:n,children:i},a))},qinline:function(e){var t=e.children,n=t.filter((function(e){return"string"!==typeof e}));if(2!==n.length)throw console.warn(t),new Error("Wrong number of children for qinline!");return Object(K.jsx)(se,{question:n[0],answer:n[1]})},question:function(e){var t=e.className,n=e.children,i=Object(u.a)(e,["className","children"]);return Object(K.jsx)("div",Object(s.a)(Object(s.a)({className:"".concat(t," mr-1")},i),{},{children:n}))},answer:function(e){var t=e.className,n=e.children,i=Object(u.a)(e,["className","children"]);return Object(K.jsx)("p",Object(s.a)(Object(s.a)({className:"".concat(t)},i),{},{children:n}))}};return Object(K.jsxs)(K.Fragment,{children:[Object(K.jsx)(ee,{description:e.description,title:e.title}),Object(K.jsxs)(Q,{children:[Object(K.jsxs)("div",{style:{margin:10,padding:10,display:"flex",flexDirection:"row",justifyContent:"center"},children:[Object(K.jsx)(oe,{href:"".concat(J,"/").concat(e.fileName,".ipynb"),children:"Download Python Notebook"}),e.dataset&&Object(K.jsx)(oe,{href:"".concat(U,"/").concat(e.dataset),children:"Download Dataset"})]}),Object(K.jsx)(O.a,{control:Object(K.jsx)(w.a,{checked:!l,onChange:function(){return d(!l)},name:"checkedA"}),label:"Show all code"}),Object(K.jsx)("article",{className:"markdown-body",style:{margin:0,padding:0},children:Object(K.jsx)(q.a,{components:f,skipHtml:!1,remarkPlugins:[B.a],rehypePlugins:[P.a,F.a],children:a})})]}),Object(K.jsx)(y.a,{variant:"h4",style:{textAlign:"center"},children:"Test your understanding"}),e.questions.map((function(e){return Object(K.jsx)(ie,Object(s.a)({},e))}))]})}var le=[{img:{url:"https://miro.medium.com/max/781/1*fGX0_gacojVa6-njlCrWZw.png",title:"Decision Tree Visual"},content:{title:"Decision Tree - ID3",description:"Learn how to implement a decision tree from scratch with Python",dataset:"heart.csv",questions:[{question:"Using ID3 tree creation, assuming 3 features are left, how many more iterations, including this one and the base case, will it take to hit and return from a base case for creating the tree, assuming each feature, even after being split up, has at least 2 unique values per class (e.g. Class A of Feature 3 would still have 2 unique values) and all information gains are 0 no matter what?",answer:"4. We'll keep going until we are out of features, so 1 for each feature and then 1 last iteration for the base case."},{question:"If we returned the highest frequency class in our target when the max information gain from any column was 0, would our accuracy scores for VALIDATION DATA be able to decrease, increase, or stay the same? Select all that apply.",answer:"The scores do any of the above.\n- Stay the same: Each branch we take after we have no more info gain may yield the same result as picking the highest frequency class.\n- Increase: An IG of 0 means there was no change in entropy. Branching again could cause entropy to change with a remaining feature (entropy can only decrease here)\n- Decrease: The opposite of the above"}]},notebook:"heart_decision_tree_classifier"},{img:{url:"https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2015/01/Decision-Tree-Example-6-Decision-tree-Edureka.png",title:"Decision Tree Visual"},content:{title:"Decision Tree - C4.5",description:"Implement decision trees from scratch using the C4.5 algorithm, which handles continuous features. Also included: min_split_count",dataset:"heart.csv",questions:[{question:"How does the C4.5 algorithm handle features with continuous values when choosing a variable to split on?",answer:"When we encounter a continuous variable while choosing a feature to split on, we iterate through all possible splits between the values in this feature. At each point, we calculate the information gained if a split was made at this point. After calculating all possible splits and the associated information gained, we choose the split that yeilds us the most information gained. At this point, we treat this column as a categorical feature based on where the split was and then continue with the normal decision tree creation.\n Note: We do not modify the actual data after finding a split, this is only used for creating the tree."},{question:"How can you determine a good value for min_split_count?",answer:"This is slightly dependent on the size of your dataset. With larger datasets, you can have higher min_split_counts that will still produce accurate trees, while smaller datasets may require smaller values (as each split will significantly reduce the amount of items remaining). You can use a testing-based approach to determine an optimal value by building trees with differing values for the min_split_count and comparing their performance using a testing dataset."}]},notebook:"heart_decision_tree_c45"},{img:{url:"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Scikit_learn_logo_small.svg/1200px-Scikit_learn_logo_small.svg.png",title:"Scitkit Learn Logo"},content:{title:"Scikit Learn Classifier",description:"Using the decision tree classifier we implemented in the previous tutorial, we will now transform that into a classifier that works with sklearn",dataset:"heart.csv",questions:[{question:"What is a cat?",answer:"An animal"},{question:"Where does wood come from?",answer:"Trees"}]},notebook:"heart_classifier_with_sklearn"},{img:{url:"https://miro.medium.com/max/1052/1*VHDtVaDPNepRglIAv72BFg.jpeg",title:"Random Forests"},content:{title:"Bagging and Random Forests",description:"Using what we know about decision trees, we will now learn how to make more powerful models by leveraging bagging and random forests",questions:[{question:"What is the difference between Bagging and Random Forest?",answer:"The primary difference between Random Forest and Bagging is that for Random Forest, only a random subset of features are used to create a tree, whereas in Bagging, the full set of features are used."},{question:"In general, would an ensemble model perform better with 2 tress or 25 trees?",answer:"It will perform better with 25 trees."},{question:"What are some advantages of Random Forest?",answer:"It helps to improve the accuracy of the model, especially when there is missing or insufficient data. Additionally, it reduces overfitting in decision trees since it accounts for variance in data by training on varied samples of the training data."}]},notebook:"bagging_and_random_forest"}],de=[{img:{url:"https://www.megahowto.com/wp-content/uploads/2009/09/Rubix-Cube.jpg",title:"a rubix cube"},content:{title:"Numpy",description:"A library for easily handling multi-dimensional arrays and matrices"},linkTo:"https://numpy.org/doc/stable/user/absolute_beginners.html"},{img:{url:"https://cameoglassuk.co.uk/wp-content/uploads/2016/07/EATING-PANDAS-1.jpg",title:"pandas eating bamboo"},content:{title:"Pandas",description:"A library for powerful data analysis and manipulation."},linkTo:"https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html"},{img:{url:"https://miro.medium.com/max/1000/1*0DDt5Xp9z6ecj5eL6FNAfQ.png",title:"data clustering example"},content:{title:"Scikit Learn",description:"A library that makes ML processes (like dimensionality reduction) easy."},linkTo:"https://scikit-learn.org/stable/tutorial/index.html"}],he=n(612),je=Object(Y.a)((function(e){return Object(Z.a)({card:{display:"flex",justifyContent:"space-between",alignItems:"center",flexWrap:"wrap",margin:"25px auto",width:"80%",padding:"12px 20px",backgroundColor:e.palette.background.paper}})}));function ue(e){var t=Object(i.useState)(!1),n=Object(g.a)(t,2),a=n[0],r=n[1],o=je();return Object(K.jsxs)(M.a,{className:o.card,elevation:a?24:6,variant:e.secondary?"outlined":"elevation",onMouseEnter:function(){return r(!0)},onMouseLeave:function(){return r(!1)},children:[Object(K.jsxs)("div",{children:[Object(K.jsx)("h2",{children:e.title}),Object(K.jsx)("p",{children:e.description})]}),Object(K.jsxs)(f.a,{component:c.b,color:"primary",variant:"contained",to:e.linkTo,children:[e.buttonText,Object(K.jsx)(he.a,{})]})]})}var be=n(613),me=n(614),pe=n(615),ge=n(616),xe=n(617),fe=Object(Y.a)((function(e){return Object(Z.a)({card:{width:"30%",minWidth:"300px",margin:"15px",display:"flex",flexDirection:"column"},media:{height:"240px",width:"100%",display:"block",objectFit:"cover",border:"3px solid",borderColor:"#4a4848"},splash:{display:"block",margin:"40px auto"}})}));function Oe(e){var t=fe();return Object(K.jsxs)(be.a,{className:t.card,children:[Object(K.jsx)(me.a,{className:t.media,image:e.img.url,title:e.img.title}),Object(K.jsxs)(pe.a,{children:[Object(K.jsx)("h3",{children:e.content.title}),Object(K.jsx)("p",{children:e.content.description})]}),Object(K.jsxs)(ge.a,{style:{display:"flex",justifyContent:"flex-end",alignItems:"flex-end",flex:"1 1 auto"},children:[e.linkTo&&Object(K.jsxs)(f.a,{color:"primary",variant:"contained",href:e.linkTo,style:{margin:10},target:"_blank",children:["View Tutorial",Object(K.jsx)(xe.a,{})]}),e.notebook&&Object(K.jsxs)(f.a,{component:c.b,color:"primary",variant:"contained",to:"/".concat(e.notebook),style:{margin:10},children:["View Tutorial",Object(K.jsx)(he.a,{})]})]})]})}var we=Object(Y.a)((function(e){return Object(Z.a)({gallery:{display:"flex",justifyContent:"space-around",flexWrap:"wrap"}})}));function ye(e){var t=we();return Object(K.jsx)("div",{className:t.gallery,children:e.cards.map((function(e){return Object(i.createElement)(Oe,Object(s.a)(Object(s.a)({},e),{},{key:e.content.title}))}))})}var ke=Object(Y.a)((function(e){return Object(Z.a)({card:{width:"30%",minWidth:"300px",margin:"10px 8px"},media:{height:"240px"},splash:{display:"block",margin:"40px auto",width:"80%",backgroundColor:"#cccccc"},backgroundImage:{position:"fixed",top:0,bottom:0,left:0,right:0,opacity:.25,zIndex:-100,height:"100%",background:"url(https://www.decadeonrestoration.org/themes/unrestore/images/tree.jpg) no-repeat center center fixed",backgroundSize:"cover"},overlay:{position:"fixed",top:0,bottom:0,left:0,right:0,opacity:.35,zIndex:-100,height:"100%",backgroundColor:"#000000",backgroundSize:"cover"}})}));function ve(){var e=ke();return Object(K.jsxs)(K.Fragment,{children:[Object(K.jsx)("div",{className:e.backgroundImage}),Object(K.jsx)("div",{className:e.overlay}),Object(K.jsxs)("section",{children:[Object(K.jsx)("h1",{children:"Your Intro to Decision Trees"}),Object(K.jsx)("p",{children:"An explainable, data-driven approach to making decisions"})]}),Object(K.jsx)(ue,{title:"Not sure where to start?",description:"Learn how to use the website",buttonText:"Get Started",linkTo:"/getting-started"}),Object(K.jsx)(ue,{title:"Unfamiliar with decision trees?",description:"Learn more about them by reading this article",buttonText:"Introduction to decision trees",linkTo:"/introduction"}),Object(K.jsx)(ue,{title:"Need a refresher?",description:"Brush up on your preliminary skills",buttonText:"Preliminary Skills",linkTo:"/preliminary-skills"}),Object(K.jsxs)("section",{children:[Object(K.jsx)("h2",{children:"Want to put it into practice?"}),Object(K.jsx)("p",{children:"Follow the guides below to implement and test powerful machine learning models to better understand decision trees"})]}),Object(K.jsx)(ye,{cards:le})]})}function Te(){return Object(K.jsxs)(K.Fragment,{children:[Object(K.jsx)(ee,{title:"Introduction to Decision Trees",description:"What are decision trees? What can they be used for?"}),Object(K.jsxs)(Q,{children:[Object(K.jsx)("h2",{children:"At a high level"}),Object(K.jsx)("p",{children:"Decision trees provide an easy-to-follow model for making a prediction given a set of features. This model is in the form of a tree, and to make a prediction, we start at the root node and follow the path given the features we want until we reach a leaf node."}),Object(K.jsx)("p",{children:"This is super abstract, so let's tie it back to reality with an everyday example!"}),Object(K.jsx)("h3",{children:"Choosing your outfit"}),Object(K.jsx)("p",{children:"Before you hop on Zoom for 5+ hours, you probably have to pick out what you're going to wear that day. There are many factors that may go into this decision including the following"}),Object(K.jsxs)("ul",{children:[Object(K.jsxs)("li",{children:["temperature ",Object(K.jsx)("em",{children:"(warm or cold)"})]}),Object(K.jsxs)("li",{children:["day of the week ",Object(K.jsx)("em",{children:"(weekday or weekend)"})]}),Object(K.jsxs)("li",{children:["how you feel ",Object(K.jsx)("em",{children:"(energized or lazy)"})]})]}),Object(K.jsx)("p",{children:"As with most decisions in life, a combination of these factors go into your final outfit. Depending on how you prioritize these factors, you may end up with a decision tree as follows."}),Object(K.jsx)("img",{src:"".concat("/csc466_project","/imgs/clothing_tree.png"),width:"100%",alt:"decision tree for deciding what to wear"}),Object(K.jsx)("p",{children:"So on a lazy sunday in January, you'd probably be wearing a hoodie and sweats."}),Object(K.jsx)("h2",{children:"Creating a tree"}),Object(K.jsx)("p",{children:"Given a data set and a goal of creating a decision tree, we start by finding features that, once a choice is made, reduces the amount of entropy within the data set the most."}),Object(K.jsx)("p",{children:"Entropy, for our case, is the measure of chaos in our predictions. We measure this by looking at the probability of outcomes given a current set of data. Our goal when choosing a feature is to minimize the overall entropy, which is equivalent to maximizing information gained with each choice. This process is repeated until either no information is gained with a choice, or there is only one outcome remaining!"}),Object(K.jsx)("p",{children:"Let's see what this looks like with a simple example"}),Object(K.jsx)("h3",{children:"Clothing data set"}),Object(K.jsx)("p",{children:"For the sake of example, assume that you have been tracking what you have been wearing for many days and you find the following results"}),Object(K.jsxs)("table",{width:"100%",children:[Object(K.jsxs)("tr",{children:[Object(K.jsx)("th",{children:"Weather"}),Object(K.jsx)("th",{children:"Day of week"}),Object(K.jsx)("th",{children:"Energy"}),Object(K.jsx)("th",{children:"Clothing"})]}),Object(K.jsxs)("tr",{children:[Object(K.jsx)("td",{children:"Warm"}),Object(K.jsx)("td",{children:"Weekday"}),Object(K.jsx)("td",{children:"Lazy"}),Object(K.jsx)("td",{children:"T-shirt"})]}),Object(K.jsxs)("tr",{children:[Object(K.jsx)("td",{children:"Warm"}),Object(K.jsx)("td",{children:"Weekend"}),Object(K.jsx)("td",{children:"Energized"}),Object(K.jsx)("td",{children:"Hawaiian shirt"})]}),Object(K.jsxs)("tr",{children:[Object(K.jsx)("td",{children:"Warm"}),Object(K.jsx)("td",{children:"Weekend"}),Object(K.jsx)("td",{children:"Lazy"}),Object(K.jsx)("td",{children:"Hawaiian shirt"})]}),Object(K.jsxs)("tr",{children:[Object(K.jsx)("td",{children:"Cold"}),Object(K.jsx)("td",{children:"Weekday"}),Object(K.jsx)("td",{children:"Lazy"}),Object(K.jsx)("td",{children:"Hoodie"})]}),Object(K.jsxs)("tr",{children:[Object(K.jsx)("td",{children:"Cold"}),Object(K.jsx)("td",{children:"Weekday"}),Object(K.jsx)("td",{children:"Energized"}),Object(K.jsx)("td",{children:"Jacket"})]}),Object(K.jsxs)("tr",{children:[Object(K.jsx)("td",{children:"Cold"}),Object(K.jsx)("td",{children:"Weekend"}),Object(K.jsx)("td",{children:"Lazy"}),Object(K.jsx)("td",{children:"Hoodie"})]}),Object(K.jsxs)("tr",{children:[Object(K.jsx)("td",{children:"Cold"}),Object(K.jsx)("td",{children:"Weekend"}),Object(K.jsx)("td",{children:"Lazy"}),Object(K.jsx)("td",{children:"Hoodie"})]})]}),Object(K.jsx)("br",{}),Object(K.jsx)("p",{children:'If we follow the idea that we\'re trying to reduce entropy (as seen in the amount of variance in the "clothing" column), we may go through the following process.'}),Object(K.jsxs)("ol",{children:[Object(K.jsx)("li",{children:"A warm or cold day makes the most difference initially. This is how we'll split the dataset."}),Object(K.jsxs)("li",{children:["On warm days, we can see that the day of the week makes more of a difference than our energy level to what we wear, so we will split from here.",Object(K.jsx)("ul",{children:Object(K.jsx)("li",{children:"Notice that once we make this split, there is no entropy in what we will wear, so we make these clothing choices leaf nodes on the tree."})})]}),Object(K.jsxs)("li",{children:["On cold days, we can see that energy level makes a much bigger impact on our clothing choice than the day of the week, so we will split the data set on this feature",Object(K.jsx)("ul",{children:Object(K.jsx)("li",{children:"Similiarily to warm days, once we make this split we have zero entropy, so we are able to make a decision (i.e. produce a leaf node)"})})]})]}),Object(K.jsx)("p",{children:"This process results in a decision tree like the one above! There is a little more math involved in the actual production, but this recursive process is what we will follow while producing our decision tree!"}),Object(K.jsx)("h2",{children:"Observations"}),Object(K.jsx)("p",{children:"We can see from the above example that decision trees are a great way to make procedural decisions in an explainable way over discrete data sets. Note that there are methods for handling continuous data, namely binning."}),Object(K.jsx)("p",{children:"We also have to be careful that our tree does not overfit our data set by becoming extremely deep (having lots of branches). This may seem like a good thing, as it will be accurate over our testing data, but this serves little use in making general decisions on new data. We can handle this best by limiting the depth of our tree or reducing the amount of features we use."})]}),Object(K.jsx)(ue,{title:"Practice time",description:"If you're ready,  get hands on experience by following our tutorials",buttonText:"See tutorials",linkTo:"/tutorials",secondary:!0})]})}function Ne(){return Object(K.jsxs)(K.Fragment,{children:[Object(K.jsx)(ee,{title:"Getting Started",description:"Here's what you'll need to follow along with the tutorials"}),Object(K.jsxs)(Q,{children:[Object(K.jsx)("h2",{children:"Learning about Decision Trees"}),Object(K.jsxs)("p",{children:["The ",Object(K.jsx)(c.b,{to:"tutorials",children:"tutorials"}),' on this site are set up in such a way that, by following along, you will learn many of the concepts behind decision trees. This is reflective of Cal Poly\'s "Learn by Doing" methodology, which (through personal observation) works quite beautifully.']}),Object(K.jsxs)("p",{children:["However, this style of learning is really only helpful if you have a basic understanding of the concepts you are trying to learn. If you're completely unfamiliar with decision trees, you should\xa0",Object(K.jsx)(c.b,{to:"introduction",children:"start here"})," to get a high level overview of what we're trying to accomplish with these. Otherwise, keep on reading to see how to follow along with the tutorials!"]}),Object(K.jsx)("h2",{children:"Running a tutorial"}),Object(K.jsxs)("p",{children:["Before we're able to work on an actual tutorial, you must first\xa0",Object(K.jsx)("a",{href:"https://jupyter.org/install#getting-started-with-the-classic-jupyter-notebook",target:"_blank",rel:"noreferrer",children:"install Jupyter Notebook"}),". After you have successfully done that, download an tutorial's python notebook, and open it using Jupyter notebook."]}),Object(K.jsx)("p",{children:"The starting files provide a clear learning path with helpful comments to guide you down the right path while building working decision trees!"})]}),Object(K.jsx)(ue,{title:"Learn more with tutorials",description:"Now that you know how to get them going, it's time to get your hands dirty with code!",buttonText:"Browse tutorials",linkTo:"/tutorials",secondary:!0})]})}function Ce(){return Object(K.jsxs)(K.Fragment,{children:[Object(K.jsx)(ee,{title:"Tutorials",description:"Get into the learn by doing spirit with these walkthrough tutorials."}),Object(K.jsx)(ye,{cards:le}),Object(K.jsx)(ee,{description:"Check the following out if you're having trouble working through the\n        tutorials",title:"Resources"}),Object(K.jsx)(ue,{title:"Unfamiliar with decision trees?",description:"Learn more about them by reading this article",buttonText:"Introduction to decision trees",linkTo:"/introduction",secondary:!0}),Object(K.jsx)(ue,{title:"Not comfortable with Pandas, Numpy, or Sklearn?",description:"Refresh your preliminary skills with easy to follow guides",buttonText:"Preliminary skills",linkTo:"/preliminary-skills",secondary:!0})]})}function _e(){return Object(K.jsxs)(K.Fragment,{children:[Object(K.jsx)(ee,{title:"Preliminary Skills",description:"Need a quick brush up on some of the KDD tools used to build decesion trees? Look no further! \nHere are links to tutorials of some commonly used tools in data science. \n"}),Object(K.jsx)(ye,{cards:de}),Object(K.jsx)("section",{children:Object(K.jsx)("p",{children:"While these won't cover everything you'll need to know, hopefully they can serve as a nice refresher."})})]})}var We=n(618),De=n(619),Se=n(620),qe=Object(Y.a)((function(e){return Object(Z.a)({heading:{color:"white"},bar:{justifyContent:"space-between"},links:{color:"white",padding:"0 12px"}})}));function Ae(){var e=qe();return Object(K.jsx)(We.a,{position:"static",children:Object(K.jsxs)(De.a,{className:e.bar,children:[Object(K.jsx)(Se.a,{to:"/",component:c.b,children:Object(K.jsx)("h3",{className:e.heading,children:"Decision Trees"})}),Object(K.jsxs)("span",{children:[Object(K.jsx)(Se.a,{className:e.links,to:"/introduction",component:c.b,children:"Introduction"}),Object(K.jsx)(Se.a,{className:e.links,to:"/tutorials",component:c.b,children:"Tutorials"}),Object(K.jsx)(Se.a,{className:e.links,to:"/preliminary-skills",component:c.b,children:"Preliminary Skills"})]})]})})}var Ie=Object(Y.a)((function(e){return Object(Z.a)({root:{overflow:"hidden",padding:"40px 0"},grid:{display:"flex",flexWrap:"wrap",justifyContent:"space-between",width:"80%",margin:"0 auto","& h3 a":{color:"white"},"& a":{display:"block",margin:"16px 0",color:"#ddd"},"& div":{display:"inline-block"}},siteTag:{width:"40%",minWidth:"300px",marginBottom:"30px"},linkTag:{width:"16.6%",minWidth:"200px",paddingRight:"3px"}})}));function Le(){var e=Ie();return Object(K.jsx)(We.a,{position:"static",component:"footer",className:e.root,children:Object(K.jsxs)("div",{className:e.grid,children:[Object(K.jsxs)("div",{className:e.siteTag,children:[Object(K.jsx)("h2",{children:"Decision Trees"}),Object(K.jsx)("p",{children:"CSC 466 | Dr. Anderson | Spring 2021 | Cal Poly"}),Object(K.jsx)(y.a,{variant:"subtitle2",children:"Ben Glossner | Ethan Zimbelman | Rupal Totale"})]}),Object(K.jsxs)("div",{className:e.linkTag,children:[Object(K.jsx)("h3",{children:"Introduction"}),Object(K.jsx)(c.b,{to:"/getting-started",children:"Getting Started"}),Object(K.jsx)(c.b,{to:"/preliminary-skills",children:"Preliminary Skills"}),Object(K.jsx)(c.b,{to:"/introduction",children:"Intro to Decision Trees"})]}),Object(K.jsxs)("div",{className:e.linkTag,children:[Object(K.jsx)("h3",{children:"Tutorials"}),le.map((function(e){return Object(K.jsx)(c.b,{to:"/".concat(e.notebook),children:e.content.title})}))]}),Object(K.jsxs)("div",{className:e.linkTag,children:[Object(K.jsx)("h3",{children:"Resources"}),Object(K.jsx)("a",{href:"https://github.com/CSC466-Team7/csc466_project",target:"_blank",rel:"noreferrer noopener",children:"GitHub"}),Object(K.jsx)("a",{href:"https://github.com/CSC466-Team7/csc466_project/tree/main/code/notebooks",target:"_blank",rel:"noreferrer noopener",children:"Notebooks"}),Object(K.jsx)("a",{href:"https://github.com/CSC466-Team7/csc466_project/tree/main/datasets",target:"_blank",rel:"noreferrer noopener",children:"Datasets"})]})]})})}n(582),n(583);var Fe=Object(d.a)({palette:{primary:{light:"#62727b",main:"#37474f",dark:"#102027",contrastText:"#fff"},secondary:{light:"#60ad5e",main:"#2e7d32",dark:"#005005",contrastText:"#000"},background:{paper:"#f3f0e8"}}});var ze=function(){return Object(K.jsx)(c.a,{children:Object(K.jsxs)(h.a,{theme:Fe,children:[Object(K.jsx)(Ae,{}),Object(K.jsx)(j.a,{children:Object(K.jsxs)(l.c,{children:[Object(K.jsx)(l.a,{exact:!0,path:"/",component:ve}),Object(K.jsx)(l.a,{path:"/introduction",component:Te}),Object(K.jsx)(l.a,{path:"/getting-started",component:Ne}),Object(K.jsx)(l.a,{path:"/preliminary-skills",component:_e}),Object(K.jsx)(l.a,{exact:!0,path:"/tutorials",component:Ce}),le.map((function(e){return Object(K.jsx)(l.a,{path:"/".concat(e.notebook),children:Object(K.jsx)(ce,Object(s.a)(Object(s.a)({},e.content),{},{fileName:e.notebook}))})}))]})}),Object(K.jsx)(Le,{})]})})},Pe=function(e){e&&e instanceof Function&&n.e(3).then(n.bind(null,627)).then((function(t){var n=t.getCLS,i=t.getFID,a=t.getFCP,r=t.getLCP,o=t.getTTFB;n(e),i(e),a(e),r(e),o(e)}))};o.a.render(Object(K.jsx)(a.a.StrictMode,{children:Object(K.jsx)(ze,{})}),document.getElementById("root")),Pe()}},[[584,1,2]]]);
//# sourceMappingURL=main.79004e49.chunk.js.map