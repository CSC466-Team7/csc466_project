{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IIBCxP6ynhLb",
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etnnlYFfjZ7w"
   },
   "source": [
    "## Ensemble model with Bagging\n",
    "\n",
    "The guiding principle behind ensemble models is to leverage a combination of weak learners to create a strong learner. Bagging does this by creating subsets of the training data through resampling, and training an ML model of choice (in our example, we will use Decision Trees) on the substes of training data. This produces numerous models, each slightly different than others. By averaging the prediction of these inidividual learners for a given obervation, we should get more robust results that accounts for variance in the test data than we would get from an individual learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MLIf65GkvE3"
   },
   "source": [
    "### Pseudocode for Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "89-1G9bafA04"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create our strong learner by bagging weak learners. Note that the code below \n",
    "will not run, it is only an outline of the general implementation.\n",
    "\"\"\"\n",
    "# Assume `data` is defined\n",
    "trees = []\n",
    "number_of_trees = 100\n",
    "for i in range(number_of_trees):\n",
    "  subset_data = resample(data)\n",
    "  tree = DecisionTreeModel().fit(subset_data)\n",
    "  trees.append(tree)\n",
    "\n",
    "\"\"\"\n",
    "Predict for target variable by running all weak learners on an observation\n",
    "and averaging the result (or taking the mode if target variable is categorical).\n",
    "\"\"\"\n",
    "# Assume `x_test` is defined where x_test is the observation we will to predict for\n",
    "results = []\n",
    "for tree in trees:\n",
    "  tree.predict(x_test)\n",
    "pred = results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APv09D-wmk30"
   },
   "source": [
    "### Bagging (Regressor) with SK Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xis6nGWxoLGh"
   },
   "source": [
    "For this example, we will load the boston home prices dataset provided by sklearn. The target variable will be the median home prices.\n",
    "\n",
    "Learn more about this dataset [here](https://scikit-learn.org/stable/datasets/toy_dataset.html#boston-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "jxpmsQi0nwxL",
    "outputId": "343747c3-3d2b-46ee-b0b0-c5838b59e4e9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0      24.0\n",
       "1      21.6\n",
       "2      34.7\n",
       "3      33.4\n",
       "4      36.2\n",
       "       ... \n",
       "501    22.4\n",
       "502    20.6\n",
       "503    23.9\n",
       "504    22.0\n",
       "505    11.9\n",
       "Length: 506, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "display(X.head())\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nD1X8WsqJxn"
   },
   "source": [
    "We will now evaluate the `mean_absolute_error` for an ensemble model with the following number of learners: 1, 10, 25, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "YHn50IkqmqJS",
    "outputId": "46f3a171-1b5b-4a84-e028-280c792d8f04"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>25</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.380392</td>\n",
       "      <td>-2.087647</td>\n",
       "      <td>-1.988549</td>\n",
       "      <td>-2.022314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.286275</td>\n",
       "      <td>-2.411765</td>\n",
       "      <td>-2.601725</td>\n",
       "      <td>-2.444353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.454902</td>\n",
       "      <td>-1.891176</td>\n",
       "      <td>-1.808863</td>\n",
       "      <td>-1.840431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.962745</td>\n",
       "      <td>-3.588039</td>\n",
       "      <td>-3.664392</td>\n",
       "      <td>-3.366784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.556863</td>\n",
       "      <td>-2.142549</td>\n",
       "      <td>-1.891843</td>\n",
       "      <td>-1.888000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1         10        25        50\n",
       "0 -3.380392 -2.087647 -1.988549 -2.022314\n",
       "1 -3.286275 -2.411765 -2.601725 -2.444353\n",
       "2 -3.454902 -1.891176 -1.808863 -1.840431\n",
       "3 -4.962745 -3.588039 -3.664392 -3.366784\n",
       "4 -2.556863 -2.142549 -1.891843 -1.888000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = {1: [], 10: [], 25: [], 50: []}\n",
    "for num_estimators in scores:\n",
    "  # define the model\n",
    "  model = BaggingRegressor(n_estimators=num_estimators)\n",
    "  # evaluate the model\n",
    "  cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "  n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "  scores[num_estimators] = n_scores\n",
    "# report performance\n",
    "scores_df = pd.DataFrame.from_dict(scores)\n",
    "display(scores_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6abyo82Rr43s"
   },
   "source": [
    "Now that we have multiple scores for various values of `n_estimators`, let's see how the number of estimators fare against each other by averaging the scores for each respective value of `n_estimators`. \n",
    "\n",
    "Here, `n_estimators` is the number of trees in the ensemble model, and we are interested in how this makes a difference to support our understanding of the benefits of bagging to create strong learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0T0qmMsrlyT",
    "outputId": "8679288d-6f80-452a-9002-67b62bc008ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    -3.252587\n",
       "10   -2.337840\n",
       "25   -2.247047\n",
       "50   -2.218703\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9BmL7o-sxO8"
   },
   "source": [
    "It is clear from the above, that as the number of estimators (i.e trees) increases, the ensemble model produces more robust predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wL27bbftUje"
   },
   "source": [
    "## Extending bagging with Random Forests\n",
    "\n",
    "Random forests is very similar to bagging, with the addition of dropping a few features in the training data (i.e only using a subset of features instead of all of them, chose randomly) for each iteration along with resampling it. This adds another level of randmness to the generationg of trees, and further account for variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StBjDKLluTu3"
   },
   "source": [
    "### Pseudocode for Random Forests\n",
    "The pseudocode below is **very** similar to the one above, except for the `resample` line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "zIJtZ0MBuOow"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create our strong learner by bagging weak learners. Note that the code below \n",
    "will not run, it is only an outline of the general implementation.\n",
    "\"\"\"\n",
    "# Assume `data` is defined\n",
    "trees = []\n",
    "number_of_trees = 100\n",
    "for i in range(number_of_trees):\n",
    "  subset_data = drop_random_features(resample(data))\n",
    "  tree = DecisionTreeModel().fit(subset_data)\n",
    "  trees.append(tree)\n",
    "\n",
    "\"\"\"\n",
    "Predict for target variable by running all weak learners on an observation\n",
    "and averaging the result (or taking the mode if target variable is categorical).\n",
    "\"\"\"\n",
    "# Assume `x_test` is defined where x_test is the observation we will to predict for\n",
    "results = []\n",
    "for tree in trees:\n",
    "  tree.predict(x_test)\n",
    "pred = results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SwF8D6Nu__E"
   },
   "source": [
    "### Random Forest (Regressor) with SK Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "LspmLNMLu8-v",
    "outputId": "f5fcefca-e41a-4190-b288-22843c0a68dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>25</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.611765</td>\n",
       "      <td>-2.184118</td>\n",
       "      <td>-1.848000</td>\n",
       "      <td>-1.870196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.288235</td>\n",
       "      <td>-2.633725</td>\n",
       "      <td>-2.332784</td>\n",
       "      <td>-2.465294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.001961</td>\n",
       "      <td>-1.844706</td>\n",
       "      <td>-1.761569</td>\n",
       "      <td>-1.761176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.907843</td>\n",
       "      <td>-3.690588</td>\n",
       "      <td>-3.386431</td>\n",
       "      <td>-3.418706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.827451</td>\n",
       "      <td>-1.878235</td>\n",
       "      <td>-1.827216</td>\n",
       "      <td>-1.787961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1         10        25        50\n",
       "0 -3.611765 -2.184118 -1.848000 -1.870196\n",
       "1 -3.288235 -2.633725 -2.332784 -2.465294\n",
       "2 -3.001961 -1.844706 -1.761569 -1.761176\n",
       "3 -3.907843 -3.690588 -3.386431 -3.418706\n",
       "4 -2.827451 -1.878235 -1.827216 -1.787961"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = {1: [], 10: [], 25: [], 50: []}\n",
    "for num_estimators in scores:\n",
    "  # define the model\n",
    "  model = RandomForestRegressor(n_estimators=num_estimators)\n",
    "  # evaluate the model\n",
    "  cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "  n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "  scores[num_estimators] = n_scores\n",
    "# report performance\n",
    "scores_df = pd.DataFrame.from_dict(scores)\n",
    "display(scores_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5LAKxXXvPZs"
   },
   "source": [
    "Let's perform the same analysis for the number of estimator that we did above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x9oL6aYwvV0P",
    "outputId": "7426c176-64fd-4a40-df90-ec3d520ba8ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    -3.196889\n",
       "10   -2.314654\n",
       "25   -2.205537\n",
       "50   -2.178932\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm6eHI4BvZX5"
   },
   "source": [
    "The results are consistent with what we observed above. Increasing the number of estimators improves the performance of the ensemble model, supporting the benefit of using an ensemble model as opposed to a single learner. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "Bagging and Random Forests.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
