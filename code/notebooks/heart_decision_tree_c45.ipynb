{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Heart Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import json\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>disease_present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  disease_present  \n",
       "0   0     1                1  \n",
       "1   0     2                1  \n",
       "2   0     2                1  \n",
       "3   0     2                1  \n",
       "4   0     2                1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/ronitf/heart-disease-uci?select=heart.csv\n",
    "\n",
    "heart_disease = pd.read_csv(\"../../datasets/heart.csv\")\n",
    "heart_disease = heart_disease.rename(columns={'target': 'disease_present'})\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## Brief Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "target_col = 'disease_present'\n",
    "t = heart_disease['disease_present']\n",
    "X2 = heart_disease.drop(columns=([target_col]))\n",
    "\n",
    "# non continuous variables are converted to strings so we can easily differentiate \n",
    "X2['sex'] = X2['sex'].astype(str)\n",
    "X2['cp'] = X2['cp'].astype(str)\n",
    "X2['fbs'] = X2['fbs'].astype(str)\n",
    "X2['restecg'] = X2['restecg'].astype(str)\n",
    "X2['exang'] = X2['exang'].astype(str)\n",
    "X2['slope'] = X2['slope'].astype(str)\n",
    "X2['ca'] = X2['ca'].astype(str)\n",
    "X2['thal'] = X2['thal'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Decision Tree (including categorical features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be implementing a decision tree that uses entropy to determine information gain for deciding when to split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If confused you're confused about what entropy and information gain is, check out the [introduction on decision trees](https://csc466-team7.github.io/csc466_project/#/introduction)\n",
    " \n",
    "The implementation details with explanations for entropy and information gain can be found in in the [Heart Decision Tree Classifier notebook](https://csc466-team7.github.io/csc466_project/#/heart_decision_tree_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculates the entropy that exists in Series.\n",
    "def entropy(y):\n",
    "    e = 0\n",
    "    for v in y.unique():\n",
    "        p_v = np.sum(y == v) / len(y)\n",
    "        total = -1 * (p_v * np.log2(p_v))\n",
    "        e += total\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculates information gain of a given feature\n",
    "def gain(y,x):\n",
    "    g = 0\n",
    "    for v in x.unique():\n",
    "        sub_t = y.iloc[np.where(x == v)]\n",
    "        g += (len(sub_t) / len(y)) * entropy(sub_t)\n",
    "    return entropy(y) - g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Best Split\n",
    "\n",
    "Now that we have a way to determine the information gain of any given feature, we can go ahead and select the best feature at an arbitrary point in our decision tree. To do this, we just need to loop over all features and select the one with the highest gain.\n",
    "\n",
    "Continuous features (such as age) pose a challenge to us though, since we have to do more than just a simple equality comparison when checking entropy. When we encounter a continuous variable, we will find go through all possible splits to find the location that gives the maximal information gain. After we find this split location, we can then (effectively) bin this feature and treat it as a categorical variable while building our tree.\n",
    "\n",
    "Finding the optimal split for continuous features must be computed each time we want to make a new split, as previous splits have likely changed the values remaining in this column. This means the optimal split of any feature is likely to be different throughout the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "quiz"
    ]
   },
   "source": [
    "<qinline>\n",
    "\n",
    "<question>\n",
    "\n",
    "    Why do we have to find a split and bin continuous features when building our tree?\n",
    "\n",
    "</question>\n",
    "\n",
    "<answer>\n",
    "\n",
    "    If we leave these features as continuous, there will be minimal information gain if this feature is chosen while building our tree. For example, consider the following.\n",
    "    \n",
    "    We have `ages=[20, 30, 40, 40, 50]` and `targets=[0, 1, 0, 1, 0]` respectively. Without computing actual information gain and for the sake of example, we can see that choosing `age=30` as our split point means that we are left with two paths. One with `age=30` (with only one person matching this criteria) and one with `age!=30` (of which, 4 people live).\n",
    "    \n",
    "    This is clearly not optimal as this process could be repeated for the remaining people in the `age!=30` category, leading to an overfitted tree. Instead, we find the use our methd of finding the split location that gives the maximal gain and bin our variables according to this.\n",
    "    \n",
    "</answer>\n",
    "\n",
    "</qinline>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the optimal feature and information gain of dataset X\n",
    "def select_split(X, y):\n",
    "    col = None\n",
    "    gr = 0\n",
    "    \n",
    "    for c in X.columns:\n",
    "        cur_gain_ratio = 0\n",
    "        \n",
    "        # if we encounter a continuous variable, find the optimal split\n",
    "        if X[c].dtypes == \"int64\" or X[c].dtypes == \"float64\":\n",
    "            c, cur_gain_ratio = cont_split(X, y, c)\n",
    "        else:\n",
    "            cur_gain_ratio = gain(y, X[c])\n",
    "        \n",
    "        if cur_gain_ratio > gr:\n",
    "            gr = cur_gain_ratio\n",
    "            col = c\n",
    "    \n",
    "    return col, gr\n",
    "\n",
    "\n",
    "def cont_split(X, y, col):\n",
    "    # order the variables in the series\n",
    "    xs = sorted(X[col].unique().tolist())\n",
    "    \n",
    "    best_split = None\n",
    "    gr = 0\n",
    "    \n",
    "    for i in range(len(xs) - 1):\n",
    "        split = round((xs[i + 1] + xs[i]) / 2, 2)\n",
    "        \n",
    "        # creates a binned series\n",
    "        s = X[col] < split\n",
    "        split_gain = gain(y, s)\n",
    "                \n",
    "        if split_gain >= gr:\n",
    "            gr = split_gain\n",
    "            best_split = split\n",
    "    \n",
    "    best_split = f'{best_split:.2f}'\n",
    "    col_name = f\"{col}<{best_split}\"\n",
    "    return col_name, gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Finds the class with the highest frequency\n",
    "def high_freq_class(y):\n",
    "    v_counts = y.value_counts()\n",
    "    max_id = v_counts.idxmax()\n",
    "    return max_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Tree\n",
    "\n",
    "Actually creating the tree at this point follows a very similiar process to what we did in the [Heart Decision Tree Classifier notebook](https://csc466-team7.github.io/csc466_project/#/heart_decision_tree_classifier). Now, we just have to handle the true and false conditions returned by our continuous splits. Before we do this though, we will be **adding** a base case!\n",
    "\n",
    "### Base Cases\n",
    "\n",
    "1. There are no more features, so we select the most common target class.\n",
    "\n",
    "2. The number of unique classes left in the target is 1. *This means that the entropy of target is 0* so we know what selection to make... the only value in the target left.\n",
    "\n",
    "3. *New:* The number of items is less than our min_split_count. So we'll return the most common target class again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "quiz"
    ]
   },
   "source": [
    "<qinline>\n",
    "\n",
    "<question>\n",
    "\n",
    "Why might we want to stop tree creation when only a certain amount of items remain?\n",
    "    \n",
    "</question>\n",
    "\n",
    "<answer>\n",
    "\n",
    "If we continue making splits when `n < min_split_count`, we run into the risk of overfitting our tree (i.e. our tree becomes too specific for the training set we provide).\n",
    "\n",
    "</answer>\n",
    "\n",
    "</qinline>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def tree_creation(X, y, min_split_count=5):\n",
    "    if len(y.unique()) == 1:\n",
    "        return y.iloc[0]\n",
    "    \n",
    "    if len(X.columns) == 0:\n",
    "        return high_freq_class(y)\n",
    "    \n",
    "    if len(y) < min_split_count:\n",
    "        return high_freq_class(y)\n",
    "    \n",
    "    col, gr = select_split(X, y)\n",
    "    \n",
    "    return tree_creation_main(X, y, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Part\n",
    "\n",
    "At this point, if we haven't hit a base case, we know we've found a valid split for our tree. This means we know want to split our tree into each class of the feature we found. After that, we need to make a new tree with the features left after the selection of this given feature. This is nearly identical to the process found in the [Heart Decision Tree Classifier notebook](https://csc466-team7.github.io/csc466_project/#/heart_decision_tree_classifier), but we must now be careful with the continous splits on continuous variables since we are checking relationships between values, not equality.  For completeness, the steps are:\n",
    "\n",
    "- Go through each unique class, `c`, in the feature, `B`\n",
    "  - Create a new observation list where the observations (targets and other features) have `B = c` in that observation\n",
    "      ```\n",
    "      # for categorical variables\n",
    "      indexes = where(B = c)\n",
    "      \n",
    "      # for continuous variables\n",
    "      indexes = where(B < c)\n",
    "      indexes = where(B >= c)\n",
    "      \n",
    "      new_X = X.locate_by_index[indexes]\n",
    "      new_y = y.locate_by_index[indexes]\n",
    "      ```\n",
    "  - Remove feature `B` from our observations\n",
    "      ```\n",
    "      new_X.drop(columns=[col])\n",
    "      ```\n",
    "  - Add to the tree at feature `B` class `c` whose value is a new tree\n",
    "     ```\n",
    "     tree[B][c] = tree(new_X, new_y)\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Creates actual decision tree\n",
    "def tree_creation_main(X,y,col):\n",
    "    tree = {col: {}}\n",
    "    \n",
    "    # continuous splits\n",
    "    if '<' in col:\n",
    "        split_vals = col.split('<')\n",
    "        col_name = split_vals[0]\n",
    "        val = float(split_vals[1])\n",
    "        \n",
    "        # True path\n",
    "        indexes = np.where(X[col_name] < val)\n",
    "        new_X = X.iloc[indexes].drop(columns=[col_name])\n",
    "        new_y = y.iloc[indexes]\n",
    "        tree[col]['True'] = tree_creation(new_X, new_y)\n",
    "        \n",
    "        # False path\n",
    "        indexes = np.where(X[col_name] >= val)\n",
    "        new_X = X.iloc[indexes].drop(columns=[col_name])\n",
    "        new_y = y.iloc[indexes]\n",
    "        tree[col]['False'] = tree_creation(new_X, new_y)\n",
    "    \n",
    "    # normal splits\n",
    "    else:\n",
    "        for v in X[col].unique():\n",
    "            indexes = np.where(X[col] == v)\n",
    "            new_X = X.iloc[indexes].drop(columns=[col])\n",
    "            new_y = y.iloc[indexes]\n",
    "            tree[col][str(v)] = tree_creation(new_X, new_y)\n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Rules\n",
    "\n",
    "The way we generate a ruleset does not change just because we have continuous variables. We create them by using the tree produced. If you're interested in seeing how we go about generating it, check out the [Heart Decision Tree Classifier notebook](https://csc466-team7.github.io/csc466_project/#/heart_decision_tree_classifier).\n",
    "\n",
    "Note: Generating a ruleset is not required. It is merely a nice way to visualize the paths created by the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Takes a tree and generates the rules of that tree used to make a prediction\n",
    "def generate_rules(tree):\n",
    "    rules = []\n",
    "    if type(tree) != dict:\n",
    "        return [[tree]]\n",
    "    for col in tree:\n",
    "        for val in tree[col]:\n",
    "            tup = (col, val)\n",
    "            generated_sub_rules = generate_rules(tree[col][val])\n",
    "            for sub_rule in generated_sub_rules:\n",
    "                new_rule = [tup]\n",
    "                new_rule.extend(sub_rule)\n",
    "                rules.append(new_rule)\n",
    "    \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Finally we are near the end! Now that we have our tree or rules, we can start to make predictions.\n",
    "\n",
    "Let's again refer to the [introduction on decision trees](https://csc466-team7.github.io/csc466_project/#/introduction). Let's say our observation is `x` and contains each feature we used to make our tree. Since `Weather` is our root feature, we need to find what `Weather` in `x`. If `x.Weather` is `warm`, we look at `x.Day of the Week` next. If that is `weekend`, we should pick `Hawaiian shirt`.\n",
    "\n",
    "But what if we get to `x.Weather` and it's `hot`? Then we have no information in our tree of what to do!!! Thus, we will just make our best guess. To do that, we can take a default value that the user provides *or* we can look at the target values up to that point and pick the class in target that occurs the most often. Otherwise, we try and recurse on the sub-tree we get when choosing the related class in the tree. If there is no classes left and we are at a prediction in the tree instead, we just return that prediction.\n",
    "\n",
    "```\n",
    "Input: x - observation\n",
    "       tree - current tree\n",
    "       default - what to return if nothing left in tree\n",
    "\n",
    "if tree.is_leaf():\n",
    "    return tree.value\n",
    "\n",
    "feature_to_use = tree.root\n",
    "observation_class = x[feature_to_use]\n",
    "\n",
    "if observation_class not in tree:\n",
    "    return default\n",
    "\n",
    "return make_prediction(tree.go_to(feature_to_use), x, default)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Returns a predicate of whether a given values matches a given rule's first feature value \n",
    "def eq_rule(val_to_match):\n",
    "    return lambda x: x[0][1] == str(val_to_match)\n",
    "\n",
    "def lt_rule(col_val):\n",
    "    # x is a \"less than\" rule. apologies for the spaghetti code\n",
    "    return lambda x: (float(col_val) < float(x[0][0].split('<')[1])) == (x[0][1] == 'True')\n",
    "\n",
    "# Used to make a prediction given a decision tree's rule and some inputs\n",
    "def make_prediction(rules,x,default):\n",
    "    if len(rules) == 0:\n",
    "        return default\n",
    "    \n",
    "    tups = []\n",
    "    next_rule = rules[0][0]\n",
    "\n",
    "    if type(next_rule) != tuple:\n",
    "        return next_rule\n",
    "    \n",
    "    col = next_rule[0]\n",
    "    \n",
    "    if '<' in col:\n",
    "        col_name, split_val = col.split('<')\n",
    "        matching_value = x[col_name]\n",
    "        filter_rule = lt_rule(matching_value)\n",
    "        \n",
    "        viable_rules = list(filter(filter_rule, rules))\n",
    "    else:\n",
    "        matching_value = x[col]\n",
    "        filter_rule = eq_rule(matching_value)\n",
    "\n",
    "        viable_rules = list(filter(filter_rule, rules))\n",
    "    \n",
    "    if len(viable_rules) == 0:\n",
    "        return default\n",
    "    \n",
    "    new_rules = list(map(lambda x: x[1:], viable_rules))\n",
    "\n",
    "    return make_prediction(new_rules, x, default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you made a dictionary for your tree, you can use the `print_tree` function below to see what your function generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to print like me :)\n",
    "def print_tree(tree):\n",
    "    mytree = copy.deepcopy(tree)\n",
    "    def fix_keys(tree):\n",
    "        if type(tree) != dict:\n",
    "            if type(tree) == np.int64:\n",
    "                return int(tree)\n",
    "        new_tree = {}\n",
    "        for key in list(tree.keys()):\n",
    "            if type(key) == np.int64:\n",
    "                new_tree[int(key)] = tree[key]\n",
    "            else:\n",
    "                new_tree[key] = tree[key]\n",
    "        for key in new_tree.keys():\n",
    "            new_tree[key] = fix_keys(new_tree[key])\n",
    "        return new_tree\n",
    "    mytree = fix_keys(mytree)\n",
    "    print(json.dumps(mytree, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your Tree\n",
    "\n",
    "- We will use `sklearn`'s train_test_split to see how we did\n",
    "\n",
    "You can find the dataset and download it from the website. After that, just set the appropriate variables!\n",
    "```\n",
    "heart_disease = pd.read_csv(\"path to heart_disease.csv\")\n",
    "X2 = heart_disease.drop(columns=[\"disease_present\"])\n",
    "t = heart_disease[\"disease_present\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = 0\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X2_train, X2_test, t_train, t_test = train_test_split(X2, t, test_size=0.3, random_state = 0)\n",
    "\n",
    "tree = tree_creation(X2_train,t_train)\n",
    "rules = generate_rules(tree)\n",
    "\n",
    "y_test = X2_test.apply(lambda x: make_prediction(rules,x,default),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does our tree look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"thal\": {\n",
      "        \"0\": 0,\n",
      "        \"1\": {\n",
      "            \"ca\": {\n",
      "                \"0\": {\n",
      "                    \"age<48.00\": {\n",
      "                        \"False\": 1,\n",
      "                        \"True\": 0\n",
      "                    }\n",
      "                },\n",
      "                \"1\": 0,\n",
      "                \"2\": 0,\n",
      "                \"3\": 0\n",
      "            }\n",
      "        },\n",
      "        \"2\": {\n",
      "            \"ca\": {\n",
      "                \"0\": {\n",
      "                    \"chol<228.50\": {\n",
      "                        \"False\": {\n",
      "                            \"oldpeak<1.65\": {\n",
      "                                \"False\": 0,\n",
      "                                \"True\": {\n",
      "                                    \"age<58.50\": {\n",
      "                                        \"False\": {\n",
      "                                            \"thalach<170.00\": {\n",
      "                                                \"False\": 1,\n",
      "                                                \"True\": {\n",
      "                                                    \"cp\": {\n",
      "                                                        \"0\": {\n",
      "                                                            \"sex\": {\n",
      "                                                                \"0\": 0,\n",
      "                                                                \"1\": 1\n",
      "                                                            }\n",
      "                                                        },\n",
      "                                                        \"2\": 1,\n",
      "                                                        \"3\": 0\n",
      "                                                    }\n",
      "                                                }\n",
      "                                            }\n",
      "                                        },\n",
      "                                        \"True\": {\n",
      "                                            \"trestbps<109.00\": {\n",
      "                                                \"False\": 1,\n",
      "                                                \"True\": 1\n",
      "                                            }\n",
      "                                        }\n",
      "                                    }\n",
      "                                }\n",
      "                            }\n",
      "                        },\n",
      "                        \"True\": 1\n",
      "                    }\n",
      "                },\n",
      "                \"1\": {\n",
      "                    \"cp\": {\n",
      "                        \"0\": 0,\n",
      "                        \"1\": 1,\n",
      "                        \"2\": 1,\n",
      "                        \"3\": 0\n",
      "                    }\n",
      "                },\n",
      "                \"2\": {\n",
      "                    \"age<63.50\": {\n",
      "                        \"False\": 1,\n",
      "                        \"True\": {\n",
      "                            \"trestbps<138.00\": {\n",
      "                                \"False\": 1,\n",
      "                                \"True\": 0\n",
      "                            }\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                \"3\": {\n",
      "                    \"thalach<149.50\": {\n",
      "                        \"False\": 1,\n",
      "                        \"True\": 0\n",
      "                    }\n",
      "                },\n",
      "                \"4\": 1\n",
      "            }\n",
      "        },\n",
      "        \"3\": {\n",
      "            \"cp\": {\n",
      "                \"0\": {\n",
      "                    \"oldpeak<0.60\": {\n",
      "                        \"False\": 0,\n",
      "                        \"True\": {\n",
      "                            \"thalach<117.00\": {\n",
      "                                \"False\": {\n",
      "                                    \"chol<217.00\": {\n",
      "                                        \"False\": 0,\n",
      "                                        \"True\": {\n",
      "                                            \"trestbps<136.00\": {\n",
      "                                                \"False\": 0,\n",
      "                                                \"True\": 1\n",
      "                                            }\n",
      "                                        }\n",
      "                                    }\n",
      "                                },\n",
      "                                \"True\": 1\n",
      "                            }\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                \"1\": {\n",
      "                    \"chol<225.00\": {\n",
      "                        \"False\": 0,\n",
      "                        \"True\": 1\n",
      "                    }\n",
      "                },\n",
      "                \"2\": {\n",
      "                    \"oldpeak<1.70\": {\n",
      "                        \"False\": 0,\n",
      "                        \"True\": {\n",
      "                            \"trestbps<124.00\": {\n",
      "                                \"False\": 0,\n",
      "                                \"True\": 1\n",
      "                            }\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                \"3\": {\n",
      "                    \"age<48.50\": {\n",
      "                        \"False\": 1,\n",
      "                        \"True\": 0\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did our decision tree do with the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7582417582417582\n",
      "F1 score: 0.7555555555555555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_test, t_test)}')\n",
    "print(f'F1 score: {f1_score(y_test, t_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this compare with the battle-tried SciKit Learn version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7912087912087912\n",
      "F1 score: 0.8080808080808081\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0, criterion='entropy', min_samples_split=5)\n",
    "model = clf.fit(X2_train, t_train)\n",
    "\n",
    "y_model_test = model.predict(X2_test)\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_model_test, t_test)}')\n",
    "print(f'F1 score: {f1_score(y_model_test, t_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not bad!\n",
    "\n",
    "The difference here is likely due to the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "quiz"
    ]
   },
   "source": [
    "<qinline>\n",
    "\n",
    "<question>\n",
    "\n",
    "How else could we handle continuous features?\n",
    "    \n",
    "</question>\n",
    "\n",
    "<answer>\n",
    "\n",
    "To avoid having to check the optimal split for continuous features at every split, we could bin these variables while preprocessing our data. This would save us a lot of computation, but would likely not result in as good of a rule set.\n",
    "\n",
    "</answer>\n",
    "\n",
    "</qinline>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "- https://en.wikipedia.org/wiki/Entropy"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
